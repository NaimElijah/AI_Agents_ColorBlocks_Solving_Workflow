import config
from agents.agent_base import AgentBase
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage
from states.state1 import AgentState


class ManagerAgent(AgentBase):
    def __init__(self, llm):
        super().__init__(llm)

    # Nodes return a “state update”, not a new state object
    async def __call__(self, state: AgentState) -> dict:
        self_res = state.get(config.self_solver_output_field)          # Get outputs from previous agents, who were called before in the graph and added to the state
        # tool_user_res = state.get(config.tools_usage_solver_output_field)   # When using ToolNode, tool outputs are NOT stored in custom fields.
        # they live in state["messages"], specifically in ToolMessage(content=...)
        # he doesn't get a message from the tool-using agent directly, but from the tool node that was called after it, we get that here later below

        # Tool result MUST be extracted from messages
        # In LangChain, tool calls are usually in 'tool_calls' attribute of AIMessage objects, but here we look for ToolMessage objects in state["messages"]
        # or 'name' attribute if it is a ToolMessage
        tool_result = None
        for msg in state.get("messages", []):
            if isinstance(msg, ToolMessage):
                tool_result = msg.content

        system_prompt = SystemMessage(
            content=(
                "You are a manager agent acting as an LLM-as-a-judge.\n"
                "Compare two solutions to the same color blocks problem.\n"
                "One solution is generated by a self-solver agent without tools, the other by a tool-using agent.\n"
                "the solution by the tool-using agent(tool_solver_agent) is obtained using a tool that calculates the optimal cost by implementing the A* search algorithm.\n"
                "The self-solver agent tries to reason about the cost without given tools help.\n"
                "Explain differences and give a short summary.\n"
                "Prefer reliable, algorithmic solutions when appropriate."
            )
        )

        user_prompt = HumanMessage(
            content=(
                "Color Blocks solution comparison:\n\n"
                f"Self-solver output:\n{self_res}\n\n"
                f"Tool-based solver output:\n{tool_result}\n\n"
                "Output in the following format:\n"
                "DIFFERENCES: <list differences>\n"
                "SUMMARY: <2-4 sentences>\n"
                "FINAL_DECISION: <which solution is more reliable and why>"
            )
        )

        response = await self.llm.ainvoke([system_prompt, user_prompt])
        # return a dictionary describing what he added or updated.
        return {
            config.manager_feedback_field: response.content,
        }



